# GPT2 pre-trained model

## What is GPT2

GPT2 was a model trained by OpenAI with the task of predicting the next word, given some starting word(s).
The model performed extremely well and the full pre-trained model has not been released. However, a "worse" version has been released to the public.
You can read more about GPT2 [here](https://openai.com/blog/better-language-models/).

It is quite a cool model as it will predict code, but also code, and latex (although it doesn't make much sense).

## How did you know how to use pre-trained gpt2?

The documentation is quite helpful, as it often gives examples. In this case, I used [this](https://pypi.org/project/pytorch-pretrained-bert/) resource to inspire myself.
I would usually start with using their sample code, then slowly adapting it to what I want. It might take some time, but it gets easier as you do it more and more!


