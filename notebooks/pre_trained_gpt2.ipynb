{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre-trained-gpt2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFtxmJuL2A9n",
        "colab_type": "text"
      },
      "source": [
        "# 1. Pre-trained GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tkg7_RJzXQf",
        "colab_type": "text"
      },
      "source": [
        "If you're using colab, keep this line as it is and run the cell.\n",
        "If you are using jupyte notebook, you can remove the line which uses `!pip` and install pytorch-pretrained-bert directly from the command line using `pip install pytorch-pretrained-bert`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N38Jave9x9UL",
        "colab_type": "code",
        "outputId": "fec318f0-6cbc-4bc8-c513-6dcc57b9001e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.243)\n",
            "Collecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.243 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.243)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609240 sha256=f01856292df7663a4a0ece932595a4ce3a6f0e2e15ca4b0949a15479e7396170\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFP216RDQ0EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT4-JSMk1pZR",
        "colab_type": "text"
      },
      "source": [
        "To find out how a pre-trained model can be imported, you can use the documentation to learn how to use it. In this case, i used this link https://pypi.org/project/pytorch-pretrained-bert/ (scroll to gpt2) to see how to use this pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWRpSWt9x6mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Gpt2_predictor:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    def predict(self, form):\n",
        "        text = self.tokenizer.encode(form['starting_tokens'])\n",
        "        encoded_text, past = torch.tensor([text]), None\n",
        "\n",
        "        for _ in range(int(form['sequence_length'])):\n",
        "            logits, past = self.model(encoded_text, past = past)\n",
        "            encoded_text = torch.multinomial(F.softmax(logits[:, -1]), 1)\n",
        "            text.append(encoded_text.item())\n",
        "        \n",
        "        return self.tokenizer.decode(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5kuSzK4yOcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2 = Gpt2_predictor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFXI7T8yYzv",
        "colab_type": "code",
        "outputId": "0ff0329e-ef2c-4646-819f-e07f2f8b19d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "gpt2.predict({\"starting_tokens\": \"once upon a time\", \"sequence_length\": 40})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'once upon a time when ancient wisdom and militarism once looked invisible in the eyes of the curious and the uninitiated.\\n\\nAt this point one expects all mankind to have come to a tentative understanding of how to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}